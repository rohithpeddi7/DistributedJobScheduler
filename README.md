# ğŸ§  Distributed Job Scheduler with Kafka and Docker

A scalable, fault-tolerant system that allows users to schedule and execute Docker-based jobs through a web interface. Jobs are processed by a pool of workers using Kafka for communication, with scheduling handled by a periodic service.

---

## ğŸš€ Features

- ğŸ–¥ **Web UI + FastAPI backend** for job submission  
- â± **Job Scheduler** service that enqueues jobs at the correct time  
- ğŸ§© **Coordinator** assigns jobs to available workers  
- ğŸ›  **Worker** processes jobs by building and running Docker containers from remote Dockerfile URLs  
- ğŸ” **Kafka topics** used for inter-service communication  
- ğŸ“¡ **Heartbeat and availability** tracking for workers  
- âœ… **Status reporting** for job execution results  
- ğŸ—ƒ **MongoDB** used to store job metadata

---

## ğŸ§± Architecture Overview

```
User â†’ FastAPI + UI â†’ MongoDB
                           â†“
                 [Scheduler - runs every min]
                           â†“
                    Kafka (jobs-queue)
                           â†“
               [Coordinator assigns jobs]
                           â†“
                Kafka (worker-{id} topic)
                           â†“
                      Worker nodes
                           â†“
                   Kafka (job-status)
```

---

## ğŸ§° Technologies Used

- ğŸ Python (FastAPI, threading, subprocess)
- ğŸ³ Docker (for executing submitted jobs)
- ğŸƒ MongoDB (for job metadata)
- ğŸ˜ Apache Kafka (message queue for coordination)
- ğŸŒ Firebase (Dockerfile hosting)
- ğŸ”„ Golang (for REST Execution Service)

---

## ğŸ“¦ Kafka Topics

| Topic Name            | Purpose                               |
|-----------------------|----------------------------------------|
| `jobs-queue`          | Scheduler â†’ Coordinator                |
| `worker-availability` | Worker â†’ Coordinator                   |
| `worker-heartbeat`    | Worker â†’ Coordinator                   |
| `worker-{id}`         | Coordinator â†’ Worker                   |
| `job-status`          | Worker â†’ Backend or Logger             |

---


## ğŸ Job Lifecycle

1. User submits job (with Dockerfile URL + scheduled time)
2. Scheduler enqueues job into Kafka at scheduled time
3. Coordinator assigns job to a free worker
4. Worker fetches Dockerfile, builds and runs it
5. Worker reports status to Kafka
6. (Optional) Backend stores the result in MongoDB

---

## ğŸ”„ Scalability

- Stateless services â†’ easy to scale horizontally
- Kafka handles high-throughput message delivery
- Workers can be autoscaled based on job volume
- MongoDB handles dynamic schemas and high insert rate
- Decoupled components make monitoring and fault isolation easy

---

## ğŸ§‘â€ğŸ’» Author

Team: __Flint and Steel__